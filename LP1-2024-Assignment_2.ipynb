{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf3653d-e405-480b-b3dc-e8c8b981d6b8",
   "metadata": {},
   "source": [
    "## Language Processing 1: assignment 2\n",
    "\n",
    "**Deadline: Nov 11, 23:59**\n",
    "\n",
    "This assignment has four parts:\n",
    "\n",
    "In the first part, you will be working on morphology, as you learned it in class. The goal is to extract words in present tense and past tense from the corpora you obtained from the first assignment (English, Spanish and a language of your choice). **You can get 20-40 points here.**\n",
    "\n",
    "In the second part, you will train some language recognizers based on language models and the corpora you obtained in the first assignment. **You can get 25 points here.**\n",
    "\n",
    "In the third part, you will have to analyze and extend the Minimum Edit Distance algorithm to return an alignment, and not only the minimum number of operations. Then, you will use that implementation to check the differences in the written forms of present and past tense words. **You can get 40 points here.**\n",
    "\n",
    "The last part will be about sentiment analysis, where you will implement the Naive Bayes model that we saw in  class. **You can get 60 points here.**\n",
    "\n",
    "You need **70 points** to pass the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73915ac-84b4-4a9f-9e86-5be45aaade83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b858e6f-22c1-4f09-b5d1-bc4e715f42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263851d-4373-4695-8ac5-f5e36bb7fa60",
   "metadata": {},
   "source": [
    "# Part 1 (Morphology):\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1.1: (10 points)\n",
    "\n",
    "Use `nltk.pos_tag` to Part-Of-Speech tag the texts in English. Use it to obtain words in present tense and in past tense.\n",
    "\n",
    "This may be helpful:\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6bcff-e2ee-4bbd-9eda-2212db0fdf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/whitesungun/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f11b3c-ed84-4569-8d8e-828849ee32e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present tense verbs (first 10): ['ponds', 'below—a', 'have', 'have', 'have', 'doubt', 'are', 'see', 'resist', 'do']\n",
      "Past tense verbs (first 10): ['lay', 'came', 'looked', 'passed', 'wanted', 'was', 'were', 'had', 'was', 'had']\n"
     ]
    }
   ],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def tokenize_nltk(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def pos_tag_tokens(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "english_text = read_file('en.txt')\n",
    "english_tokens = tokenize_nltk(english_text)\n",
    "tagged_tokens = pos_tag_tokens(english_tokens)\n",
    "\n",
    "present_tense_verbs = [word for word, tag in tagged_tokens if tag in ['VBP', 'VBZ']]\n",
    "past_tense_verbs = [word for word, tag in tagged_tokens if tag in ['VBD']]\n",
    "\n",
    "print(\"Present tense verbs (first 10):\", present_tense_verbs[:10])\n",
    "print(\"Past tense verbs (first 10):\", past_tense_verbs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3fd06-a075-4acf-8e92-32f409265ef5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1.2: (10 points)\n",
    "\n",
    "#### Extra: If you train your own model, you get +10 points\n",
    "\n",
    "Get a pos-tagger for Spanish and use it to obtain words in present tense and in past tense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65112c6d-2cb0-44d4-9e66-f393143176d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Using cached numpy-2.0.2-cp312-cp312-macosx_11_0_arm64.whl (13.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[33mWARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.0.2\n",
      "\u001b[33mWARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f047f3fd-5a1b-41f4-8d70-b3ac304f422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[33mWARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Skipping /opt/anaconda3/lib/python3.12/site-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6559495-0757-447a-bfae-4195e9cac037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "201c6dfc-231c-4ed3-bf11-ebe6229682c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple está buscando comprar una startup del Reino Unido por mil millones de dólares. Los coches autónomos delegan la responsabilidad del seguro en sus fabricantes. San Francisco analiza prohibir los robots de reparto. Londres es una gran ciudad del Reino Unido. El gato come pescado. Veo al hombre con el telescopio. La araña come moscas. El pingüino incuba en su nido sobre el hielo. ¿Dónde estáis? ¿Quién es el presidente francés? ¿Dónde se encuentra la capital de Argentina? ¿Cuándo nació José de San Martín?\n",
      "buscando No tense information\n",
      "comprar No tense information\n",
      "delegan ['Pres']\n",
      "analiza ['Pres']\n",
      "prohibir No tense information\n",
      "come ['Pres']\n",
      "Veo ['Pres']\n",
      "come ['Pres']\n",
      "incuba ['Imp']\n",
      "estáis ['Pres']\n",
      "encuentra ['Pres']\n",
      "nació ['Past']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.es.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "material = \"\"\n",
    "\n",
    "material = ' '.join(sentences)\n",
    "\n",
    "doc = nlp(material)\n",
    "print(doc.text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"VERB\":\n",
    "        tense = token.morph.get('Tense')\n",
    "        if tense:\n",
    "            print(token.text, tense)\n",
    "        else:\n",
    "            print(token.text, \"No tense information\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4c73d-6c34-45fa-8333-662a706628ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1.3 (optional): (10 points)\n",
    "\n",
    "Do it for your extra language. Get a pos-tagger and obtain words in present and past tense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b541668e-e1b5-4dcb-a338-20b5117af008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.udpipe in /opt/anaconda3/lib/python3.12/site-packages (1.3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ufal.udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9ea4444-3d9d-4ad5-9433-9511652727cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ufal.udpipe\n",
      "Version: 1.3.1.1\n",
      "Summary: Bindings to UDPipe library\n",
      "Home-page: https://ufal.mff.cuni.cz/udpipe\n",
      "Author: Milan Straka\n",
      "Author-email: straka@ufal.mff.cuni.cz\n",
      "License: MPL 2.0\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show ufal.udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc0502-3ebf-40a0-a1ef-6ed5eecb3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ufal.udpipe\n",
    "\n",
    "model = ufal.udpipe.Model.load('ukrainian-ud-2.5-191206.udpipe')\n",
    "\n",
    "with open('ukr.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "processor = ufal.udpipe.Pipeline(model, 'tokenize', 'lemma', 'upos', 'conllu')\n",
    "processed_text = processor.process(text)\n",
    "\n",
    "print(processed_text)\n",
    "\n",
    "from ufal.udpipe import ProcessingError, Sentence\n",
    "\n",
    "def get_tense_verbs(text):\n",
    "    sentence = Sentence()\n",
    "    sentence.from_conllu(text)\n",
    "    present_verbs = []\n",
    "    past_verbs = []\n",
    "\n",
    "    for word in sentence.words:\n",
    "        if word.upos == \"VERB\":  \n",
    "            if word.feats:\n",
    "                feats = word.feats.split(\"|\")\n",
    "                for feat in feats:\n",
    "                    if \"Tense=Pres\" in feat:  \n",
    "                        present_verbs.append(word.form)\n",
    "                    elif \"Tense=Past\" in feat: \n",
    "                        past_verbs.append(word.form)\n",
    "\n",
    "        return present_verbs, past_verbs\n",
    "\n",
    "present, past = get_tense_verbs(processed_text)\n",
    "print(\"Present Tense Verbs:\", present)\n",
    "print(\"Past Tense Verbs:\", past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac1536-1314-405e-8a4e-a3024ed19c63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2 (Language Models)\n",
    "\n",
    "## Exercise 2.1 (15 points):\n",
    "\n",
    "Train a language recognizer using character bigrams (pairs of adjacent characters) in the text data. You should train one Language Model for each text that you obtained in the first assignment (English, Spanish and the other language of your choice).\n",
    "\n",
    "You can use the NLTK library to create a conditional frequency distribution  (`nltk.ConditionalFreqDist`) for bigrams in the data. This  counts the frequency of each pair of adjacent characters in the text.\n",
    "\n",
    "Then, you should convert this frequency distribution into a conditional probability distribution (`nltk.ConditionalProbDist`) using Maximum Likelihood Estimation (`nltk.MLEProbDist`), where each entry represents the probability of a character given its preceding character.\n",
    "\n",
    "Using this method, you should obtain three Language Models, which can be used to return a probability of a sequence to belong to each language, given a new sequence of letters (for instance, a sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9623bc-f4b9-46ee-8402-d7ed5460f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/whitesungun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Probability: 1.8352319598025105e-13\n",
      "Spanish Probability: 1.9955440512212427e-17\n",
      "Ukrainian Probability: 9.999999999999994e-78\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "import nltk\n",
    "from nltk import bigrams, ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('en.txt', 'r', encoding='utf-8') as f:\n",
    "    english_text = f.read()\n",
    "with open('sp.txt', 'r', encoding='utf-8') as f:\n",
    "    spanish_text = f.read()\n",
    "with open('ukr.txt', 'r', encoding='utf-8') as f:\n",
    "    ukrainian_text = f.read()\n",
    "\n",
    "def get_cfd(text):\n",
    "    text_bigrams = list(bigrams(text))  \n",
    "    return ConditionalFreqDist(text_bigrams)\n",
    "\n",
    "cfd_english = get_cfd(english_text)\n",
    "cfd_spanish = get_cfd(spanish_text)\n",
    "cfd_ukrainian = get_cfd(ukrainian_text)\n",
    "\n",
    "def get_cpd(cfd):\n",
    "    return ConditionalProbDist(cfd, MLEProbDist)\n",
    "\n",
    "cpd_english = get_cpd(cfd_english)\n",
    "cpd_spanish = get_cpd(cfd_spanish)\n",
    "cpd_ukrainian = get_cpd(cfd_ukrainian)\n",
    "\n",
    "def calculate_sequence_probability(text, cpd):\n",
    "    probability = 1.0\n",
    "    for bigram in bigrams(text.lower()):\n",
    "        char1, char2 = bigram\n",
    "        if cpd[char1].prob(char2) > 0:\n",
    "            probability *= cpd[char1].prob(char2)\n",
    "        else:\n",
    "            probability *= 1e-7\n",
    "    return probability\n",
    "\n",
    "test_sentence = \"Good morning\"\n",
    "\n",
    "english_prob = calculate_sequence_probability(test_sentence, cpd_english)\n",
    "spanish_prob = calculate_sequence_probability(test_sentence, cpd_spanish)\n",
    "ukrainian_prob = calculate_sequence_probability(test_sentence, cpd_ukrainian)\n",
    "\n",
    "print(f\"English Probability: {english_prob}\")\n",
    "print(f\"Spanish Probability: {spanish_prob}\")\n",
    "print(f\"Ukrainian Probability: {ukrainian_prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8aea3-01f1-4e64-9193-add441703ffa",
   "metadata": {},
   "source": [
    "## Exercise 2.2 (10 points)\n",
    "\n",
    "Provide 3 examples of sentences and their probability to belong to each language. Do you think that these Language Models are good enough to recognize the language of a text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824f2cb-7652-4429-97fd-aad6cdf380b6",
   "metadata": {},
   "source": [
    "These language models, based on character bigrams, are effective for recognizing short texts and distinguishing between languages like English, Spanish, and Ukrainian. However, their accuracy decreases with longer or more complex texts, as they lack contextual understanding. For better performance, more advanced models, such as word-based n-grams or deep learning, could be used to capture broader language patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90078982-d0ec-4881-9bc2-afa6b46101b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: 'Hello, how are you?'\n",
      " English Probability: 3.705544053835486e-21\n",
      " Spanish Probability: 5.230677018956097e-28\n",
      " Ukrainian Probability: 1.181764548951039e-116\n",
      "\n",
      " Sentence: 'Buenos días, ¿cómo estás?'\n",
      " English Probability: 2.0851180042065683e-76\n",
      " Spanish Probability: 7.069983864512341e-28\n",
      " Ukrainian Probability: 9.930367504835577e-162\n",
      "\n",
      " Sentence: 'Доброго ранку, як справи?'\n",
      " English Probability: 9.245584597804221e-162\n",
      " Spanish Probability: 9.478065682702833e-162\n",
      " Ukrainian Probability: 5.960867225275129e-28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Buenos días, ¿cómo estás?\",\n",
    "    \"Доброго ранку, як справи?\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    english_prob = calculate_sequence_probability(sentence, cpd_english)\n",
    "    spanish_prob = calculate_sequence_probability(sentence, cpd_spanish)\n",
    "    ukrainian_prob = calculate_sequence_probability(sentence, cpd_ukrainian)\n",
    "    print(f\" Sentence: '{sentence}'\")\n",
    "    print(f\" English Probability: {english_prob}\")\n",
    "    print(f\" Spanish Probability: {spanish_prob}\")\n",
    "    print(f\" Ukrainian Probability: {ukrainian_prob}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a84be-7ddb-49db-af4d-45fda6c8634b",
   "metadata": {},
   "source": [
    "# Part 3 (Minimum Edit Distance)\n",
    "\n",
    "In this exercise, we give you an implementation of Minimum Edit Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2725f7-b0db-41e6-8494-1528de2d500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1 1 - 2.0 2.0 0.0 s\n",
      "1.0 1 2 - 3.0 1.0 3.0 d\n",
      "2.0 1 3 - 4.0 2.0 4.0 d\n",
      "3.0 1 4 - 5.0 3.0 5.0 d\n",
      "4.0 1 5 - 6.0 4.0 4.0 d\n",
      "1.0 2 1 - 1.0 3.0 3.0 i\n",
      "2.0 2 2 - 2.0 2.0 2.0 i\n",
      "3.0 2 3 - 3.0 3.0 3.0 i\n",
      "2.0 2 4 - 4.0 4.0 2.0 s\n",
      "3.0 2 5 - 5.0 3.0 5.0 d\n",
      "2.0 3 1 - 2.0 4.0 2.0 i\n",
      "3.0 3 2 - 3.0 3.0 3.0 i\n",
      "4.0 3 3 - 4.0 4.0 4.0 i\n",
      "3.0 3 4 - 3.0 5.0 5.0 i\n",
      "2.0 3 5 - 4.0 4.0 2.0 s\n",
      "[[5. 4. 3. 2.]\n",
      " [4. 3. 2. 3.]\n",
      " [3. 2. 3. 4.]\n",
      " [2. 1. 2. 3.]\n",
      " [1. 0. 1. 2.]\n",
      " [0. 1. 2. 3.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ops=['i','d','s']\n",
    "\n",
    "def MED (src_str,trg_str):\n",
    "    src_str = \"#\"+src_str\n",
    "    trg_str = \"#\"+trg_str\n",
    "    \n",
    "    ins_cost = 1\n",
    "    del_cost = 1\n",
    "    sub_cost = 2\n",
    "    \n",
    "    m = len(src_str)\n",
    "    n = len(trg_str)\n",
    "    \n",
    "    distance_matrix = np.zeros((n,m))\n",
    "\n",
    "    distance_matrix [:,0] = np.arange(0,n,del_cost)\n",
    "    \n",
    "    distance_matrix [0,:] = np.arange(0,m,ins_cost)    \n",
    "    \n",
    "    for i in range(1,n): \n",
    "        for j in range(1,m): \n",
    "            insert = distance_matrix[i-1,j] + ins_cost\n",
    "            delete = distance_matrix[i,j-1] + del_cost\n",
    "            if src_str[j]==trg_str[i]:\n",
    "                substi = distance_matrix[i-1,j-1]\n",
    "            else:\n",
    "                substi = distance_matrix[i-1,j-1] + sub_cost\n",
    "\n",
    "            distance_matrix[i,j] = min([insert,delete,substi])\n",
    "            which_op = np.argmin([insert,delete,substi])\n",
    "            \n",
    "            print (distance_matrix[i,j],i,j, \"-\",insert,delete,substi, ops[which_op])\n",
    "            \n",
    "    print (np.flip(distance_matrix.T, axis=0))\n",
    "    \n",
    "    return distance_matrix[-1,-1]\n",
    "\n",
    "MED (\"PRNAP\",\"PAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cb932-7901-4427-9947-31a932e158e9",
   "metadata": {},
   "source": [
    "### Exercise 3.1 (20 pts):\n",
    "\n",
    "You have to extend the given implementation so that it returns one alignment. Considering one minimum edit distance, there might be more than one alignment. If you return one possible alignment, the exercise will be considered correct.\n",
    "\n",
    "The alignment should be a sequence of actions that should be applied to the source string. Considering the source string \"intention\" and the target string \"execution\", the function could return\n",
    "\n",
    "`['d','s','s','-','i','s','-','-','-','-']`\n",
    "\n",
    "where `d` represents deletion, `i` represents insertion, `s` represents substitution and `-` represents no change. This result could be used to represent the two aligned strings as in Figure 3.23 in SLP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fc410c9-2c17-4eb1-ae9b-01f494ac27a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.0, ['s', 'i', 's', '', 's', 'd', 'i', 'i', 'i', 'i', 's', 's', 's', 's'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MED_alignment(src_str, trg_str):\n",
    "    src_str = \"#\"+src_str\n",
    "    trg_str = \"#\"+trg_str\n",
    "    \n",
    "    ins_cost = 1\n",
    "    del_cost = 1\n",
    "    sub_cost = 2\n",
    "    \n",
    "    m = len(src_str)\n",
    "    n = len(trg_str)\n",
    "    \n",
    "    distance_matrix = np.zeros((n,m))\n",
    "    distance_matrix [:,0] = np.arange(0,n,del_cost)\n",
    "    distance_matrix [0,:] = np.arange(0,m,ins_cost)\n",
    "    \n",
    "    bck = np.zeros((n,m),dtype=str)\n",
    "    ops=['i','d','s']\n",
    "    \n",
    "    for i in range(1,n): \n",
    "        for j in range(1,m): #each row\n",
    "            insert = distance_matrix[i-1,j] + ins_cost\n",
    "            delete = distance_matrix[i,j-1] + del_cost\n",
    "            if src_str[j]==trg_str[i]:\n",
    "                substi = distance_matrix[i-1,j-1]\n",
    "            else:\n",
    "                substi = distance_matrix[i-1,j-1] + sub_cost\n",
    "\n",
    "            distance_matrix[i,j] = min([insert,delete,substi])\n",
    "            which_op = np.argmin([insert,delete,substi])\n",
    "            bck[i,j] = ops[which_op]\n",
    "            \n",
    "            #print (distance_matrix[i,j],i,j, \"-\",insert,delete,substi, ops[which_op])\n",
    "        \n",
    "    #YOUR CODE HERE\n",
    "    alignment = []\n",
    "    i, j = n - 1, m - 1\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        operation = bck[i, j]\n",
    "        alignment.append(operation)\n",
    "        \n",
    "        if operation == 'i':\n",
    "            i -= 1  \n",
    "        elif operation == 'd':\n",
    "            j -= 1  \n",
    "        else:  \n",
    "            i -= 1\n",
    "            j -= 1 \n",
    "\n",
    "    alignment.reverse()\n",
    "    \n",
    "    return distance_matrix[-1, -1], alignment\n",
    "\n",
    "    #If you uncomment the following command you can see\n",
    "    #the distance matrix in the same way\n",
    "    #that appears in Figure 3.27 from the SLP2 book, page 111.\n",
    "#    print (np.flip(distance_matrix.T, axis=0))\n",
    "    \n",
    "    #RETURN THE LAST ELEMENT\n",
    "    return distance_matrix[-1,-1]\n",
    "\n",
    "MED_alignment (\"intention\",\"execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882a471-30d8-47b1-9df2-364bb0a4e252",
   "metadata": {},
   "source": [
    "### Exercise 3.2: (20 points)\n",
    "\n",
    "Use your MED method to return the path to see the difference between verbs that are in the present tense and in the past tense. Create a corpus of approximately 10 word pairs (present/past) obtained from the first three exercises. Use MED to see the difference between the present and past tense.\n",
    "\n",
    "The examples could look like: `play-played`, `run-ran`, `como-comí`, ...\n",
    "\n",
    "What do you observe? Can you find patterns using this method? Are there differences between the languages? What about your language of choice (optional)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5eeeaa7-0f54-427f-a406-3aa9b15763c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, ['s', 's', 's', 's', 'i', 'i'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English Word Pairs\n",
    "MED_alignment (\"play\",\"played\")\n",
    "MED_alignment (\"run\",\"ran\")\n",
    "MED_alignment (\"walk\",\"walked\")\n",
    "MED_alignment (\"eat\",\"ate\")\n",
    "MED_alignment (\"jump\",\"jumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e2e0a0-a4b5-4572-b61e-eab1ac525d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, ['s', 's', 's', 'd', 'd', 'i', 'i'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spanish Word Pairs \n",
    "\n",
    "MED_alignment (\"comer\",\"comí\")\n",
    "MED_alignment (\"hablar\",\"hablé\")\n",
    "MED_alignment (\"vivir\",\"viví\")\n",
    "MED_alignment (\"correr\",\"corrí\")\n",
    "MED_alignment (\"jugar\",\"jugué\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e450b00b-8599-4f3c-9f88-0606d2dd7599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, ['s', 'd', 'd', 'i', 'i', 'i'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ukrainian Word Pairs \n",
    "\n",
    "MED_alignment (\"пишу\", \"писав\") # write\n",
    "MED_alignment (\"читаю\", \"читав\")# read\n",
    "MED_alignment (\"говорю\", \"говорив\")# speak\n",
    "MED_alignment (\"бачу\", \"бачив\") # see\n",
    "MED_alignment (\"йду\", \"йшов\")# go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00989805-4980-4da4-8512-c2c1925999bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "English Word Pairs\n",
    "MED: 2.0\n",
    "Operations: Primarily substitutions and insertions.\n",
    "Pattern: Minor changes, with fewer substitutions or insertions, indicating some verbs may only differ by small character changes.\n",
    "\n",
    "Spanish Word Pairs\n",
    "MED: 4.0\n",
    "Operations: Mix of substitutions, deletions, and insertions.\n",
    "Pattern: Generally more edits needed, with suffixes in present and past forms differing in a predictable way, aligning with the regular conjugation patterns in Spanish.\n",
    "\n",
    "Ukrainian Word Pairs\n",
    "MED: 5.0\n",
    "Operations: Mostly substitutions and deletions.\n",
    "Pattern: Higher MED score suggests larger morphological changes between present and past tenses. The frequent deletions align with more complex conjugation rules.\n",
    "\n",
    "Summary:\n",
    "English requires fewer edits, reflecting more irregular changes.\n",
    "Spanish uses consistent suffix replacements, shown in moderate MED and balanced operations.\n",
    "Ukrainian has the highest MED, reflecting more extensive verb transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2d801-142d-4d26-b299-740726ed6b91",
   "metadata": {},
   "source": [
    "# Part 4 (Sentiment Analysis)\n",
    "\n",
    "#### Naive Bayes in movie review data\n",
    " \n",
    " * Pang, B., Lee, L., & Vaithyanathan, S. (2002, July). [Thumbs up?: sentiment classification using machine learning techniques.](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf) In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10 (pp. 79-86). Association for Computational Linguistics.\n",
    " \n",
    " * You can download the data from [this website](http://www.cs.cornell.edu/people/pabo/movie-review-data/) (There are different versions, let's all download the 1.1 version of the polarity dataset (`polarity dataset v1.1 (2.2Mb) (includes README.1.1):...`))\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "In this exercise you will implement the model that was presented in the lecture about Sentiment Analysis. The Naive Bayes model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a21a5953-6f59-4bd5-90f2-c569b2e08874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3d3ac0-963c-45a7-aa49-6d9ea798d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filetowordlist(path, sfx):\n",
    "    words = []\n",
    "    for item in sorted(os.listdir(path)):\n",
    "        if sfx in item:\n",
    "            f=open(path + item, encoding=\"iso8859-1\")\n",
    "            lines = [line.strip() for line in f]\n",
    "            f.close()\n",
    "            wordsinfile = []\n",
    "            for l in lines:\n",
    "                sentencewords = l.split()\n",
    "                wordsinfile = wordsinfile + sentencewords\n",
    "            words.append(wordsinfile)\n",
    "    return words\n",
    "\n",
    "def log(number):\n",
    "    return np.log(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be7678-479c-4936-912f-a6facebd90d8",
   "metadata": {},
   "source": [
    "##### Note: YOU MUST ADJUST THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5836ebc2-443d-425a-99e7-c2430e4979b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the directory below\n",
    "#Put the directory where you downloaded the corpus\n",
    "\n",
    "posreviews_all = filetowordlist(\"mix20_rand700_tokens_0211/tokens/pos/\", \".txt\")\n",
    "negreviews_all = filetowordlist(\"mix20_rand700_tokens_0211/tokens/neg/\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24b02456-ed71-41e1-98a9-c860223d425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set 550 out of the positive reviews and 550 out of the negative reviews for creating a model\n",
    "#We will teach the model how to make predictions using that portion of the data\n",
    "posreviews_train = posreviews_all[:550]\n",
    "negreviews_train = negreviews_all[:550]\n",
    "\n",
    "#And later, we will be able to use the remaining part to see how well our model performs\n",
    "#This is called the test data\n",
    "posreviews_test  = posreviews_all[550:]\n",
    "negreviews_test  = negreviews_all[550:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d21466af-1ecd-4d63-afc6-facb7f052e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 550, 144, 142)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the number of positive and negative reviews in each of the portions of the data (train and test)\n",
    "len(posreviews_train), len(negreviews_train),len(posreviews_test), len(negreviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56b5fb30-df15-460e-9581-56d20038d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are two lists with all words in each group of reviews\n",
    "#poswords_train contains a list of words, with concatenated positive reviews\n",
    "poswords_train=[word for sent in posreviews_train for word in sent]\n",
    "negwords_train=[word for sent in negreviews_train for word in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f84fa-9b46-4d18-8c94-7868f5e704e5",
   "metadata": {},
   "source": [
    "Look at a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f537f539-c245-43fa-956b-77bad743457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'novels', '\"', \"carlito's\", 'way', '\"', 'and', '\"', 'after', 'hours', '\"', 'by', 'edwin', 'torres', ')', 'starring', ':', 'al', 'pacino', ',', 'sean', 'penn', ',', 'penelope', 'ann', 'miller', ',', 'john', 'leguiziamo', ',', 'luis', 'guzman', ',', 'john', 'rebhorn', ',', 'viggio', 'mortensen', ',', 'jorge', 'porcel', \"what's\", 'shocking', 'about', '\"', \"carlito's\", 'way', '\"', 'is', 'how']\n"
     ]
    }
   ],
   "source": [
    "print (poswords_train[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f51f28a5-cb0e-4827-b1d6-786fa2d8221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'being', 'semi-consoled', 'by', 'brooke', ',', 'one', 'of', 'his', 'graduate', 'students', '.', 'he', 'lives', 'on', 'arlington', 'road', 'where', 'he', 'makes', 'friends', 'with', 'neighbors', 'oliver', '(', 'robbins', ')', 'and', 'cheryl', 'lang', '(', 'cusack', ')', '.', 'then', 'he', 'begins', 'to', 'suspect', 'that', 'oliver', 'is', 'not', 'the', 'architect', 'he', 'claims', 'to', 'be', ',']\n"
     ]
    }
   ],
   "source": [
    "print (negwords_train[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b0a5957-201f-4004-89d1-977c0158793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabularies for positive and negative reviews\n",
    "pos_vocab_train = set(poswords_train)\n",
    "neg_vocab_train = set(negwords_train)\n",
    "vocab_train = pos_vocab_train.union(neg_vocab_train)\n",
    "\n",
    "#Number of types (vocabulary size)\n",
    "pos_vocab_size_train = len(pos_vocab_train)\n",
    "neg_vocab_size_train = len(neg_vocab_train)\n",
    "vocab_size_train = len(vocab_train)\n",
    "\n",
    "#Number of words (tokens)\n",
    "noposwords_train=len(poswords_train)\n",
    "nonegwords_train=len(negwords_train)\n",
    "\n",
    "#Number of reviews\n",
    "noposreviews_train=len(posreviews_train)\n",
    "nonegreviews_train=len(negreviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11273b30-5d34-466e-8e5f-5ef206cbeaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27813, 445289, 550)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab_size_train,noposwords_train,noposreviews_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "376b6173-0bbb-4fca-ac70-27d37e9485d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6094379124341003, 0.6989700043360189)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(5),np.log10(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a97159-12f9-4b47-b24a-4f345e0c6c3e",
   "metadata": {},
   "source": [
    "#### Exercise 4.1 (12 pts):\n",
    "Calculate prior probabilities, which are the probabilities of $P(C=positive)$ and $P(C=negative)$.\n",
    "\n",
    "Instead of saving probabilities, remember the practical issue that we mentioned at the end of the class (we save logprobs (f.ex. `np.log`) instead of regular probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34767799-9812-42a3-8f37-58eea44be1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the log probability of a review to be positive:\n",
      "-0.6931471805599453\n",
      "This is the log probability of a review to be negative:\n",
      "-0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "total_reviews_train = noposreviews_train + nonegreviews_train\n",
    "\n",
    "prior_probabiolity_pos_train = np.log(noposreviews_train / total_reviews_train)\n",
    "prior_probabiolity_neg_train = np.log(nonegreviews_train / total_reviews_train)\n",
    "\n",
    "print (\"This is the log probability of a review to be positive:\")\n",
    "print (prior_probabiolity_pos_train)\n",
    "print (\"This is the log probability of a review to be negative:\")\n",
    "print (prior_probabiolity_neg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bd018-4996-4fec-8f01-bb0bd4a768a8",
   "metadata": {},
   "source": [
    "##### Tricky part:\n",
    "\n",
    "Now we need to estimate the probability of a given word, given a specific class.\n",
    "\n",
    "$$P\\left(w_{k} | c_{i}\\right)=\\frac{\\operatorname{count}\\left(w_{k}, c_{i}\\right)+1}{\\sum_{w \\in V} \\operatorname{count}\\left(w, c_{i}\\right) + |V|}$$\n",
    "\n",
    "$$ logP\\left(w_{k} | c_{i}\\right) = log (P\\left(w_{k} | c_{i}\\right))$$\n",
    "\n",
    "We need some word counts, then.\n",
    "\n",
    "#### Exercise 4.2 (12 pts):\n",
    "Calculate word counts for the positive reviews and save them in a variable.\n",
    "\n",
    "Calculate also the word counts for the negative reviews and save them in another variable.\n",
    "\n",
    "##### Hint: You can use a dictionary (for each document class or polarity) to save the word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b4ab4fe-54ac-40b4-a74e-23933f9e9c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word nice appears 118 times in the positive reviews\n",
      "The word nice appears 92 times in the negative reviews\n",
      "\n",
      "The word bad appears 223 times in the positive reviews\n",
      "The word bad appears 509 times in the negative reviews\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from collections import Counter\n",
    "\n",
    "pos_frequencies = Counter(poswords_train)\n",
    "neg_frequencies = Counter(negwords_train)\n",
    "    \n",
    "print (\"The word nice appears \"+str(pos_frequencies[\"nice\"])+\" times in the positive reviews\")\n",
    "print (\"The word nice appears \"+str(neg_frequencies[\"nice\"])+\" times in the negative reviews\")\n",
    "\n",
    "print ()\n",
    "\n",
    "print (\"The word bad appears \"+str(pos_frequencies[\"bad\"])+\" times in the positive reviews\")\n",
    "print (\"The word bad appears \"+str(neg_frequencies[\"bad\"])+\" times in the negative reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7731e0-31c4-4561-8428-51eb0df18096",
   "metadata": {},
   "source": [
    "#### Exercise 4.3 (12 pts):\n",
    "Now estimate the probability of each word to appear in a positive review. Do the same with negative reviews. When you save them, do as you did in the first exercise, and save log probabilities.\n",
    "\n",
    "##### Hint: You can use a dictionary (for each document class or polarity) to save the word logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b0ffd3b-1078-4eea-9189-1d7f7e3acc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability of the word nice to appear in the positive reviews is: -8.310737353530826\n",
      "The log probability of the word nice to appear in the negative reviews is: -8.438475770037291\n",
      "\n",
      "The log probability of the word bad to appear in the positive reviews is: -7.678214794787315\n",
      "The log probability of the word bad to appear in the negative reviews is: -6.736664537472175\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "vocab_size_train = len(vocab_train)\n",
    "noposwords_train = len(poswords_train)\n",
    "nonegwords_train = len(negwords_train)\n",
    "\n",
    "pos_logprobs = {}\n",
    "neg_logprobs = {}\n",
    "\n",
    "for word in vocab_train:\n",
    "    pos_prob = (pos_frequencies[word] + 1) / (noposwords_train + vocab_size_train)\n",
    "    pos_logprobs[word] = np.log(pos_prob)\n",
    "    \n",
    "    neg_prob = (neg_frequencies[word] + 1) / (nonegwords_train + vocab_size_train)\n",
    "    neg_logprobs[word] = np.log(neg_prob)\n",
    "\n",
    "\n",
    "print (\"The log probability of the word nice to appear in the positive reviews is: \"+str(pos_logprobs[\"nice\"]))\n",
    "print (\"The log probability of the word nice to appear in the negative reviews is: \"+str(neg_logprobs[\"nice\"]))\n",
    "\n",
    "print ()\n",
    "\n",
    "print (\"The log probability of the word bad to appear in the positive reviews is: \"+str(pos_logprobs[\"bad\"]))\n",
    "print (\"The log probability of the word bad to appear in the negative reviews is: \"+str(neg_logprobs[\"bad\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07a33e-afb0-4e25-91be-ab37e684773d",
   "metadata": {},
   "source": [
    "#### Exercise 4.4 (12 pts):\n",
    "When making predictions, we need to handle out-of-vocabulary words carefully. Typically, if a word is not present in the training corpus, it's simply ignored. However, this approach can lead to issues when a word is unique to one class of documents (e.g., only appearing in positive examples).\n",
    " \n",
    "To avoid these problems and ensure a clean implementation, we estimate the log probability of unseen words for both positive and negative classes in our training set. Please save these two probabilities in two variables called `pos_oov_word_logprob` and `neg_oov_word_logprob`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbae6a01-e8af-44ec-b3c6-ede924624b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability of an out of vocabulary word in the positive reviews is:\n",
      "-13.089860846642354\n",
      "The log probability of an out of vocabulary word in the negative reviews is:\n",
      "-12.971075263190547\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "pos_oov_word_logprob = np.log(1 / (noposwords_train + vocab_size_train))\n",
    "neg_oov_word_logprob = np.log(1 / (nonegwords_train + vocab_size_train))\n",
    "\n",
    "print (\"The log probability of an out of vocabulary word in the positive reviews is:\")\n",
    "print (pos_oov_word_logprob)\n",
    "print (\"The log probability of an out of vocabulary word in the negative reviews is:\")\n",
    "print (neg_oov_word_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763c931-1321-4d7d-8023-3dcb5ddd3a9b",
   "metadata": {},
   "source": [
    "#### Exercise 4.5 (12 pts):\n",
    "\n",
    "Now we just need to write a function that will return True if an input review is positive, and False if the input review is negative.\n",
    "\n",
    "$$\\operatorname{argmax}\\left(\\log P\\left(c_{i}\\right)+\\sum_{k=1}^{N} \\log P\\left(w_{k} | c_{i}\\right)\\right)$$\n",
    "\n",
    "How would you do that?\n",
    "\n",
    "##### Hint: Keep it simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0e0c075-e15c-47ce-b528-c2c584b602e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_or_not(s):\n",
    "    log_prob_pos = prior_probabiolity_pos_train\n",
    "    log_prob_neg = prior_probabiolity_neg_train\n",
    "\n",
    "    for word in s:\n",
    "        log_prob_pos += pos_logprobs.get(word, pos_oov_word_logprob)\n",
    "        log_prob_neg += neg_logprobs.get(word, neg_oov_word_logprob)\n",
    "\n",
    "    return log_prob_pos > log_prob_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f41291e-7a15-4e3f-9915-29c53c9989dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"the movie is nice\".split()\n",
    "positive_or_not(review_test) #This is supposed to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25a161a6-aa58-4395-a336-5da34ae08c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"the movie is bad\".split()\n",
    "positive_or_not(review_test) #This is supposed to be negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "414f5ada-4b27-431a-9cea-0467c59d334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"such an awful movie\".split()\n",
    "positive_or_not(review_test) #This is supposed to be negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a8d9514-60e9-4a00-bb58-8c8613e1514e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"lovely experience\".split()\n",
    "positive_or_not(review_test) #This is supposed to be positive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
